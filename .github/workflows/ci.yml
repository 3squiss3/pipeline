name: CI

on:
  pull_request:
  push:
    branches: ["main"]
  schedule:
    - cron: "0 6 * * 1"
  workflow_dispatch:

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install ruff
        run: pip install ruff

      - name: Ruff lint
        run: ruff check scripts spark/jobs dags --output-format=github

      - name: Ruff format (check)
        run: ruff format --check scripts spark/jobs dags

  build_scan_spark:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4

      - uses: docker/setup-buildx-action@v3

      - name: Build spark image (cached)
        uses: docker/build-push-action@v6
        with:
          context: ./spark
          load: true
          tags: demo-spark:ci
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Trivy scan (HIGH/CRITICAL fail)
        uses: aquasecurity/trivy-action@0.24.0
        with:
          image-ref: demo-spark:ci
          format: table
          exit-code: "1"
          severity: "CRITICAL,HIGH"

  integration:
    runs-on: ubuntu-latest
    needs: build_scan_spark
    steps:
      - uses: actions/checkout@v4

      - name: Compose up (sans alerter)
        run: |
          docker compose -p pipelines up -d --build \
            mongo airflow-db airflow-init airflow spark

      - name: Wait for Mongo
        run: |
          for i in {1..30}; do
            docker compose -p pipelines exec -T mongo \
              mongosh "mongodb://root:rootpass@mongo:27017/admin?authSource=admin" \
              --quiet --eval "db.adminCommand('ping').ok" | grep 1 && exit 0
            sleep 2
          done
          echo "Mongo not ready" && docker compose -p pipelines logs mongo && exit 1

      - name: Run pipelines (seed -> generate -> publish -> spark)
        run: |
          docker compose -p pipelines exec -T airflow python /opt/airflow/scripts/00_seed.py
          docker compose -p pipelines exec -T airflow python /opt/airflow/scripts/00b_generate_prices.py 60
          docker compose -p pipelines exec -T airflow python /opt/airflow/scripts/02_pipeline2_publish.py
          docker compose -p pipelines exec -T spark spark-submit /opt/spark-app/jobs/03_pipeline3_metrics_spark.py

      - name: Assert expected counts
        run: |
          OUT="$(docker compose -p pipelines exec -T airflow python /opt/airflow/scripts/99_check.py)"
          echo "$OUT"
          echo "$OUT" | grep -E "products:\s*3"
          echo "$OUT" | grep -E "price_events:\s*60"
          echo "$OUT" | grep -E "product_views:\s*3"
          echo "$OUT" | grep -E "metrics_snapshot:\s*3"
          echo "$OUT" | grep -E "alerts:\s*0"

      - name: Logs on failure
        if: failure()
        run: docker compose -p pipelines logs --no-color

      - name: Compose down
        if: always()
        run: docker compose -p pipelines down -v
